{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc9be56",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3b52",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67da3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n",
      "2  homelessness  or houselessness as george carli...\n",
      "3  airport    starts as a brand new luxury    pla...\n",
      "4  brilliant over  acting by lesley ann warren . ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b946",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f07be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (15000, 10000)\n",
      "Validation data shape: (5000, 10000)\n",
      "Test data shape: (5000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(reviews, Y, test_size=0.4, random_state=42, stratify=Y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(X_train[0]).toarray()\n",
    "X_val = vectorizer.transform(X_val[0]).toarray()\n",
    "X_test = vectorizer.transform(X_test[0]).toarray()\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07ee9",
   "metadata": {},
   "source": [
    "**(b)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "576c0192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n",
      "Words and their counts in the first review:\n",
      "about: 1\n",
      "action: 1\n",
      "adventure: 1\n",
      "adventures: 1\n",
      "all: 2\n",
      "am: 1\n",
      "an: 1\n",
      "and: 8\n",
      "anticipation: 1\n",
      "anyone: 1\n",
      "as: 2\n",
      "bad: 2\n",
      "become: 1\n",
      "been: 1\n",
      "bond: 1\n",
      "but: 1\n",
      "by: 1\n",
      "camp: 1\n",
      "character: 1\n",
      "characters: 1\n",
      "check: 1\n",
      "choices: 1\n",
      "crew: 2\n",
      "decided: 1\n",
      "disjointed: 1\n",
      "doc: 5\n",
      "elements: 2\n",
      "etc: 1\n",
      "even: 1\n",
      "familiar: 2\n",
      "fan: 1\n",
      "fanatic: 1\n",
      "fans: 1\n",
      "feel: 1\n",
      "feeling: 1\n",
      "film: 2\n",
      "first: 1\n",
      "for: 2\n",
      "from: 1\n",
      "good: 1\n",
      "have: 1\n",
      "here: 1\n",
      "hero: 1\n",
      "heroes: 1\n",
      "his: 2\n",
      "how: 1\n",
      "in: 5\n",
      "indiana: 1\n",
      "inspiration: 1\n",
      "into: 1\n",
      "is: 4\n",
      "it: 2\n",
      "james: 1\n",
      "jones: 1\n",
      "just: 2\n",
      "know: 1\n",
      "long: 1\n",
      "lot: 1\n",
      "major: 1\n",
      "many: 1\n",
      "minutes: 1\n",
      "more: 1\n",
      "movie: 3\n",
      "movies: 1\n",
      "music: 1\n",
      "my: 1\n",
      "not: 2\n",
      "number: 2\n",
      "of: 9\n",
      "one: 2\n",
      "ones: 1\n",
      "only: 1\n",
      "other: 1\n",
      "ought: 1\n",
      "out: 1\n",
      "overwhelming: 1\n",
      "promise: 1\n",
      "provided: 1\n",
      "really: 1\n",
      "respond: 1\n",
      "resulting: 1\n",
      "savage: 1\n",
      "say: 1\n",
      "see: 1\n",
      "seeing: 1\n",
      "should: 1\n",
      "so: 1\n",
      "somewhat: 1\n",
      "spirit: 1\n",
      "star: 2\n",
      "superman: 1\n",
      "that: 4\n",
      "the: 11\n",
      "their: 1\n",
      "them: 2\n",
      "then: 1\n",
      "there: 2\n",
      "they: 1\n",
      "this: 3\n",
      "those: 1\n",
      "throw: 1\n",
      "time: 1\n",
      "to: 7\n",
      "trek: 2\n",
      "trying: 1\n",
      "unfortunate: 1\n",
      "us: 1\n",
      "was: 1\n",
      "way: 2\n",
      "what: 2\n",
      "who: 2\n",
      "why: 1\n",
      "winded: 1\n",
      "with: 2\n",
      "you: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "review_vector = X_train[0]\n",
    "\n",
    "nonzero_indices = review_vector.nonzero()[0]\n",
    "print(\"Words and their counts in the first review:\")\n",
    "\n",
    "for idx in nonzero_indices:\n",
    "    print(f\"{vocab[idx]}: {review_vector[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2638fce",
   "metadata": {},
   "source": [
    "**(c)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "751d357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m160,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m34\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,050</span> (625.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,050\u001b[0m (625.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,050</span> (625.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m160,050\u001b[0m (625.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 10)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    layers.Input(shape=(10000,)), \n",
    "    layers.Dense(units=16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1863637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7950 - loss: 0.5482 - val_accuracy: 0.8596 - val_loss: 0.5092\n",
      "Epoch 2/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8748 - loss: 0.4800 - val_accuracy: 0.8596 - val_loss: 0.5319\n",
      "Epoch 3/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8746 - loss: 0.5004 - val_accuracy: 0.8614 - val_loss: 0.5498\n",
      "Epoch 4/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8723 - loss: 0.4973 - val_accuracy: 0.8640 - val_loss: 0.5375\n",
      "Epoch 5/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8867 - loss: 0.4902 - val_accuracy: 0.8250 - val_loss: 0.6043\n",
      "Epoch 6/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8724 - loss: 0.4937 - val_accuracy: 0.8590 - val_loss: 0.5605\n",
      "Epoch 7/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8814 - loss: 0.4995 - val_accuracy: 0.8484 - val_loss: 0.5816\n",
      "Epoch 8/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8735 - loss: 0.5151 - val_accuracy: 0.8608 - val_loss: 0.5373\n",
      "Epoch 9/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8797 - loss: 0.4917 - val_accuracy: 0.8500 - val_loss: 0.5659\n",
      "Epoch 10/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8784 - loss: 0.4953 - val_accuracy: 0.8646 - val_loss: 0.5410\n",
      "Epoch 11/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8801 - loss: 0.4828 - val_accuracy: 0.8684 - val_loss: 0.5194\n",
      "Epoch 12/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8764 - loss: 0.4867 - val_accuracy: 0.8674 - val_loss: 0.5402\n",
      "Epoch 13/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8795 - loss: 0.4737 - val_accuracy: 0.8490 - val_loss: 0.5641\n",
      "Epoch 14/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8772 - loss: 0.4895 - val_accuracy: 0.8538 - val_loss: 0.5367\n",
      "Epoch 15/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8748 - loss: 0.4874 - val_accuracy: 0.8380 - val_loss: 0.5528\n",
      "Epoch 16/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8752 - loss: 0.4743 - val_accuracy: 0.8512 - val_loss: 0.5524\n",
      "Epoch 17/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8778 - loss: 0.4821 - val_accuracy: 0.8534 - val_loss: 0.5715\n",
      "Epoch 18/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8769 - loss: 0.4893 - val_accuracy: 0.8564 - val_loss: 0.5452\n",
      "Epoch 19/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8795 - loss: 0.4810 - val_accuracy: 0.8564 - val_loss: 0.5391\n",
      "Epoch 20/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8816 - loss: 0.4707 - val_accuracy: 0.8534 - val_loss: 0.5527\n",
      "Epoch 21/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8736 - loss: 0.4924 - val_accuracy: 0.8464 - val_loss: 0.5811\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf33fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# modelMLP = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=10, random_state=42)\n",
    "# modelMLP.fit(X_train, y_train.values.flatten())\n",
    "\n",
    "# y_train_pred = modelMLP.predict(X_train)\n",
    "# print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# y_val_pred = modelMLP.predict(X_val)\n",
    "# print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd327a6",
   "metadata": {},
   "source": [
    "**(d)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "056af421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 - 0s - 2ms/step - accuracy: 0.8526 - loss: 0.5512\n",
      "Test Loss: 0.551157534122467\n",
      "Test Accuracy: 0.8525999784469604\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44ee62",
   "metadata": {},
   "source": [
    "**(e)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1ef2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Sentence: 'I loved this movie! It was amazing and thrilling.' -> Sentiment: positive\n",
      "Sentence: 'The film was terrible, I regret watching it.' -> Sentiment: negative\n",
      "Sentence: 'Pretty average, not too bad but not great either.' -> Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "custom_reviews = [\n",
    "    \"I loved this movie! It was amazing and thrilling.\",\n",
    "    \"The film was terrible, I regret watching it.\",\n",
    "    \"Pretty average, not too bad but not great either.\"\n",
    "]\n",
    "\n",
    "# Transform using the already-fitted vectorizer\n",
    "X_custom = vectorizer.transform(custom_reviews).toarray()\n",
    "\n",
    "# Predict with the trained model\n",
    "predictions = model.predict(X_custom)\n",
    "\n",
    "# Get predicted classes (0 or 1)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "\n",
    "# Map 0/1 to 'negative'/'positive'\n",
    "label_map = {0: \"negative\", 1: \"positive\"}\n",
    "classified_sentences = [label_map[label] for label in predicted_labels]\n",
    "\n",
    "# Print the results\n",
    "for sentence, label in zip(custom_reviews, classified_sentences):\n",
    "    print(f\"Sentence: '{sentence}' -> Sentiment: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
